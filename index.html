<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EMMP</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://seungyeon-k.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://dsqnet.github.io">
            [T-ASE 2022] DSQNet
          </a>
          <a class="navbar-item" href="https://sqpdnet.github.io">
            [CoRL 2022] SQPDNet
          </a>
          <a class="navbar-item" href="https://searchforgrasp.github.io">
            [CoRL 2023] Search-for-Grasp
          </a>
        </div>
      </div>
    </div>

  </div> -->
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <!-- Title. -->
          <h1 class="title is-2 publication-title">Equivariant Motion Manifold Primitives</h1>
          <p class="subtitle">Conference on Robot Learning (CoRL) 2023</p>
          
          <!-- Authors. -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Byeongho Lee<sup>*, 1</sup>,</span>
            <span class="author-block">
              <a href="https://www.gabe-yhlee.com">Yonghyeon Lee</a><sup>*, 2</sup>,
            <span class="author-block">
              <a href="https://seungyeon-k.github.io">Seungyeon Kim</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Minjun Son<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/robotics.snu.ac.kr/fcp/">Frank C. Park</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University,</span>
            <span class="author-block"><sup>2</sup>Korea Institute For Advanced Study</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <!-- Icons. -->
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=psyvs5wdAV"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=asoUlHj2vnQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dlsfldl/emmp-public"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1s3ZmuiIWnQXRpZZvrfxGueqm95xa1yu9/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset (Toy2D)</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1nYkC_-DH-x6-eE69Fc-gN2W5nDtzAp2J/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset (Pouring)</span>
                  </a>
              </span>

            </div>
          </div>
          
          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>

          <!-- TL;DR. -->
          <div class="columns is-centered has-text-justified interpolation-panel">
            <div class="column is-full-width">
              <h2 class="title is-4">
                <font color="#808080">
                  <p></p><p></p><p></p>
                  TL;DR: This paper propose a new family of highly adaptable primitive models, Equivariant Motion Manifold Primitives (EMMP), which consider inherent symmetry in the robot tasks.
                </font>
              </h2>
            </div>
          </div>
          
          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>


        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing movement primitive models for the most part focus on representing and 
            generating a single trajectory for a given task, limiting their adaptability
            to situations in which unforeseen obstacles or new constraints may arise.
            In this work we propose Motion Manifold Primitives (MMP), a movement primitive
            paradigm that encodes and generates, for a given task, a continuous manifold
            of trajectories each of which can achieve the given task. To address the
            challenge of learning each motion manifold from a limited amount of data,
            we exploit inherent symmetries in the robot task by constructing motion
            manifold primitives that are equivariant with respect to given symmetry groups.
            Under the assumption that each of the MMPs can be smoothly deformed into each
            other, an autoencoder framework is developed to encode the MMPs and also
            generate solution trajectories. Experiments involving synthetic and real-robot
            examples demonstrate that our method outperforms existing manifold primitive
            methods by significant margins. 
            Code is available at \url{https://github.com/dlsfldl/EMMP-public}.
          </p>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/watch?v=asoUlHj2vnQ"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <!-- <div class="content has-text-centered">
          <p>
            Coming soon
          </p>
        </div>         -->
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Illustration. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motion Manifold Primitives (MMP)</h2>
        <!-- Extended Deformable Superquadrics. -->
        <h3 class="title is-4">Manifold-Based Trajectory Learning</h3>
        <div class="content has-text-justified">
          <p>
            There exist more than one trajectory that can perform a given task, specified by <b>a task parameter \(\tau\).</b>
            Assuming that the trajectories form a manifold, our goal is to learn the manifold of trajectories for each task, 
            denoted by a <b>Motion Manifold \({\cal M}_\tau\).</b>
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/edsq.png"
                alt="deformable superquadrics"/>
          </div>
        </div>
        <!-- TablewareNet. -->
        <h3 class="title is-4">TablewareNet: Dataset for Cluttered Transparent Tableware</h3>
        <div class="content has-text-justified">
          <p>
              We combine deformable superquadrics to define templates representing seven types of tableware: <b>wine glasses, 
              bottles, beer bottles, bowls, dishes, handleless cups</b>, and <b>mugs</b>. By adjusting parameters, we can generate 
              diverse 3D tableware meshes. Spawning these meshes in a user-defined environment (e.g., table or shelf) within 
              a physics simulator allows us to generate cluttered scenes. Using Blender, a photorealistic renderer, with transparent 
              textures, we obtain RGB images of the scenes from arbitrary camera poses.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="tablewarenet" autoplay controls muted loop playsinline height="100%">
          <!-- <video id="tablewarenet" controls muted preload playsinline width="75%"> -->
            <source src="./assets/videos/tablewarenet.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Recognition Model. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Recognition Model</h2>

        <!-- T2SQNet. -->
        <h3 class="title is-4">T<sup>2</sup>SQNet: Transparent Tableware SuperQuadric Network</h3>
        <div class="content has-text-justified">
          <p>
            Overall, our method consists of four steps: (1) mask prediction in 2D images, (2) prediction of 3D bounding boxes, 
            (3) computation of a <b>smoothed visual hull</b> through voxel carving, and (4) prediction of tableware parameters 
            (i.e., a set of superquadric parameters). We apply these modules sequentially during inference, which can lead to 
            the accumulation of prediction errors. To address this, we develop techniques to train each module accurately and 
            robustly against noise and sim-to-real gaps.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="tablewarenet" autoplay controls muted loop playsinline height="100%">
          <!-- <video id="tablewarenet" controls muted preload playsinline width="75%"> -->
            <source src="./assets/videos/t2sqnet.mp4"
                    type="video/mp4">
          </video>
        </div>

        <!-- Recognition Results. -->
        <h3 class="title is-4">Recognition Results</h3>
        <div class="content has-text-justified">
          <p>
            The figure below shows the ground-truth shapes of the transparent TRansPose objects alongside the 
            inferred implicit surfaces from T<sup>2</sup>SQNet. Although capturing surface details, such as the 
            curvature of a water bottle, is challenging due to the nature of superquadric surfaces, we can confirm
             that T<sup>2</sup>SQNet infers the overall shapes to a considerable extent.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/recognition.png"
                 alt="recognition results"/>
          </div>
        </div>
   
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Geometry-aware Object Manipulation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Geometry-Aware Object Manipulation</h2>

        <!-- Object Manipulation with T2SQNet. -->
        <h3 class="title is-4">Object Manipulation with T<sup>2</sup>SQNet</h3>
        <div class="content has-text-justified">
          <p>
            T<sup>2</sup>SQNet offers several practical advantages for downstream object manipulation tasks. 
            For example, it allows for the easy design of an effective 6-DoF grasp sampler based on deformable 
            superquadric representations, enables rapid collision checks through implicit function representations 
            of deformable superquadric surfaces, and facilitates target-driven manipulation with instance-wise 
            object recognition.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/manipulation.png"
                 alt="manipulation"/>
          </div>
        </div>


        <!-- Object Manipulation Results. -->
        <h3 class="title is-4">Object Manipulation Results</h3>
        <div class="content has-text-justified">
          <p>
            We demonstrate the effectiveness of our model, T<sup>2</sup>SQNet, on two object manipulation tasks: (i) 
            <b>sequential decluttering</b>, which involves sequential grasping in a cluttered environment, and (ii) 
            <b>target retrieval</b>, which involves object rearrangement planning to retrieve an initially non-graspable 
            target object. The target object is indicated by a specific tableware class name (e.g., wineglass). Real-world 
            manipulation videos can be found below.
          </p>
        </div>

        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/6m5ZOrbSxxI?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Citation. -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <pre><code>
@inproceedings{kim2024t2sqnet,
  title={T$^2$SQNet: A Recognition Model for Manipulating Partially Observed Transparent Tableware Objects},
  author={Kim, Young Hun and Kim, Seungyeon and Lee, Yonghyeon and Park, Frank C},
  booktitle={8th Annual Conference on Robot Learning}
}
    </code></pre>
  </div>
</section>

<!-- Footer. -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template modified from <a href="https://nerfies.github.io/">NeRFies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
